import streamlit as st
import openai
import streamlit as st
from dotenv import load_dotenv
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback
import os
from langchain.chains import ConversationChain
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)
from streamlit_chat import message  # Importez la fonction message
import toml
import docx2txt
from langchain.memory.chat_message_histories import StreamlitChatMessageHistory
import docx2txt
from dotenv import load_dotenv
if 'previous_question' not in st.session_state:
    st.session_state.previous_question = []

# Chargement de l'API Key depuis les variables d'environnement
load_dotenv(st.secrets["OPENAI_API_KEY"])

# Configuration de l'historique de la conversation
if 'previous_questions' not in st.session_state:
    st.session_state.previous_questions = []

st.markdown(
    """
    <style>

        .user-message {
            text-align: left;
            background-color: #E8F0FF;
            padding: 8px;
            border-radius: 15px 15px 15px 0;
            margin: 4px 0;
            margin-left: 10px;
            margin-right: -40px;
            color:black;
        }

        .assistant-message {
            text-align: left;
            background-color: #F0F0F0;
            padding: 8px;
            border-radius: 15px 15px 15px 0;
            margin: 4px 0;
            margin-left: -10px;
            margin-right: 10px;
            color:black;
        }

        .message-container {
            display: flex;
            align-items: center;
        }

        .message-avatar {
            font-size: 25px;
            margin-right: 20px;
            flex-shrink: 0; /* Emp√™cher l'avatar de r√©tr√©cir */
            display: inline-block;
            vertical-align: middle;
        }

        .message-content {
            flex-grow: 1; /* Permettre au message de prendre tout l'espace disponible */
            display: inline-block; /* Ajout de cette propri√©t√© */
}
        .message-container.user {
            justify-content: flex-end; /* Aligner √† gauche pour l'utilisateur */
        }

        .message-container.assistant {
            justify-content: flex-start; /* Aligner √† droite pour l'assistant */
        }
        input[type="text"] {
            background-color: #E0E0E0;
        }

        /* Style for placeholder text with bold font */
        input::placeholder {
            color: #555555; /* Gris fonc√© */
            font-weight: bold; /* Mettre en gras */
        }

        /* Ajouter de l'espace en blanc sous le champ de saisie */
        .input-space {
            height: 20px;
            background-color: white;
        }
    
    </style>
    """,
    unsafe_allow_html=True
)
# Sidebar contents
textcontainer = st.container()
with textcontainer:
    logo_path = "medi.png"
    logoo_path = "NOTEPRESENTATION.png"
    st.sidebar.image(logo_path,width=150)
   
    
st.sidebar.subheader("Suggestions:")
questions = [
        "Donnez-moi un r√©sum√© du rapport ",
        "Qu'est-ce qu'un datacenter ?",
        "Comment le gouvernement marocaine soutient-il les projets de transformation digitale dans le pays ?",
        "Quelles sont les pr√©visions de croissance du march√© de datacenters au Maroc jusqu'en 2026 ?",
        "Quels d√©fis le Maroc doit-il relever pour devenir un hub technologique africain dans le domaine des Datacenters ?"  ]    
load_dotenv(st.secrets["OPENAI_API_KEY"])
# Initialisation de l'historique de la conversation dans `st.session_state`
if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = StreamlitChatMessageHistory()
def main():
    conversation_history = StreamlitChatMessageHistory()  # Cr√©ez l'instance pour l'historique

    st.header("Rapport Monographie sectorielle : Les Datacenters üí¨")
    
    # Load the document
    docx = 'monographie.docx'
    
    if docx is not None:
        # Lire le texte du document
        text = docx2txt.process(docx)

        # Afficher toujours la barre de saisie
        st.markdown('<div class="input-space"></div>', unsafe_allow_html=True)
        selected_questions = st.sidebar.radio("****Choisir :****", questions)
        # Afficher toujours la barre de saisie
        query_input = st.text_input("", key="text_input_query", placeholder="Posez votre question ici...", help="Posez votre question ici...")
        st.markdown('<div class="input-space"></div>', unsafe_allow_html=True)

        if query_input and query_input not in st.session_state.previous_question:
            query = query_input
            st.session_state.previous_question.append(query_input)
        elif selected_questions:
            query = selected_questions
        else:
            query = ""

        if query :
            st.session_state.conversation_history.add_user_message(query)  # Ajouter √† l'historique
            if "Donnez-moi un r√©sum√© du rapport" in query:
                summary = "Le rapport Monographie sectorielle d√©taille le secteur des Datacenters au Maroc et dans le monde, en mettant en lumi√®re le march√© en pleine croissance, les technologies cl√©s, et les strat√©gies de d√©veloppement √† l'√©chelle nationale et internationale. Il aborde l'essor du march√© marocain des Datacenters, soulignant son potentiel de croissance gr√¢ce √† la strat√©gie nationale du num√©rique et l'ambition du Maroc de devenir un hub technologique africain. Le document explore √©galement les r√©cents d√©veloppements du secteur √† l'√©chelle mondiale, y compris les tendances, les technologies de pointe, et l'importance des Datacenters dans la transformation digitale. Des analyses sp√©cifiques sur l'infrastructure, les services offerts, et les d√©fis tels que la s√©curit√© des donn√©es et l'efficacit√© √©nerg√©tique sont inclus, ainsi que des perspectives sur les opportunit√©s d'investissement et les implications pour le futur du secteur."
                st.session_state.conversation_history.add_ai_message(summary) 


        
            else:
                messages = [
                {
                    "role": "user",
                    "content": (
                        f"{query}. En tenant compte du texte suivant essayer de ne pas dire le texte ne contient pas les informations si ne trouve pas √† partir de texte r√©pondre d'apr√©s votre connaissance stp et ne dire pas stp le texte incomplet ou incompr√©hensible essayer de formul√© une bon r√©ponse sans critiquer le texte par exemple ne pas dire texte fragmenter ou quelque chose comme √ßa r√©pondre directement stp parceque je vais l'afficher o lecteur: {text} "
                    )
                }
            ]

            # Appeler l'API OpenAI pour obtenir le r√©sum√©
                response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=messages
            )

            # R√©cup√©rer le contenu de la r√©ponse

                summary = response['choices'][0]['message']['content']
            
                # Votre logique pour traiter les r√©ponses
            #conversation_history.add_user_message(query)
            #conversation_history.add_ai_message(response)
                st.session_state.conversation_history.add_ai_message(summary)  # Ajouter √† l'historique
            
            # Afficher la question et le r√©sum√© de l'assistant
            #conversation_history.add_user_message(query)
            #conversation_history.add_ai_message(summary)

            # Format et afficher les messages comme pr√©c√©demment
            formatted_messages = []
            previous_role = None 
            if st.session_state.conversation_history.messages: # Variable pour stocker le r√¥le du message pr√©c√©dent
                    for msg in conversation_history.messages:
                        role = "user" if msg.type == "human" else "assistant"
                        avatar = "üßë" if role == "user" else "ü§ñ"
                        css_class = "user-message" if role == "user" else "assistant-message"

                        if role == "user" and previous_role == "assistant":
                            message_div = f'<div class="{css_class}" style="margin-top: 25px;">{msg.content}</div>'
                        else:
                            message_div = f'<div class="{css_class}">{msg.content}</div>'

                        avatar_div = f'<div class="avatar">{avatar}</div>'
                
                        if role == "user":
                            formatted_message = f'<div class="message-container user"><div class="message-avatar">{avatar_div}</div><div class="message-content">{message_div}</div></div>'
                        else:
                            formatted_message = f'<div class="message-container assistant"><div class="message-content">{message_div}</div><div class="message-avatar">{avatar_div}</div></div>'
                
                        formatted_messages.append(formatted_message)
                        previous_role = role  # Mettre √† jour le r√¥le du message pr√©c√©dent

                    messages_html = "\n".join(formatted_messages)
                    st.markdown(messages_html, unsafe_allow_html=True)

if __name__ == '__main__':
    main()
